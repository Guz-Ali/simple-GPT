# simple-GPT
Text Generation with a Decoder-only Transformer, trained on Shakespeare

This model is an implementation of the 'Attention is All You Need' paper, trained on a very tiny-tiny dataset compared to current datasets that are being used for LLM training.

This project was inspired by and followed from [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY) by [Andrej Karpathy on YouTube](https://www.youtube.com/@AndrejKarpathy). 
