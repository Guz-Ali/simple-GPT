# simple-GPT
Text Generation with a Decoder-only Transformer, trained on Shakespeare

This model is an implementation of the 'Attention is All You Need' paper, trained on a very tiny-tiny dataset compared to current datasets that are being used for LLM training.
