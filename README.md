# simple-GPT

A project where we fine-tune on fineweb-edu10B with a replica of the GPT 2 model with the hyperparameters from GPT 3 and the tokenizer from GPT 4. Trained on 8 A100 SXM4 (40GB) GPUs on [Lambda Labs](https://lambdalabs.com/) for one epoch, approx. 3 hours training time.

This project is inspired by 
- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY) 
- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
- [Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU)

by [Andrej Karpathy on YouTube](https://www.youtube.com/@AndrejKarpathy). 

---
